{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "b5819a4f-627e-46c7-b41e-e5544e9f32b8",
      "metadata": {
        "id": "b5819a4f-627e-46c7-b41e-e5544e9f32b8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class Context:\n",
        "    \"\"\"Context to save variables for backward computation.\"\"\"\n",
        "    def __init__(self):\n",
        "        self.saved_tensors = ()\n",
        "    def save_for_backward(self, *tensors):\n",
        "        self.saved_tensors = tuple(tensors)\n",
        "\n",
        "class Op:\n",
        "    @staticmethod\n",
        "    def forward(ctx, *args):\n",
        "        pass\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        pass\n",
        "\n",
        "    @classmethod\n",
        "    def apply(cls, *inputs):\n",
        "        ctx = Context()\n",
        "        raw_inputs = [x.data if isinstance(x, Tensor) else x for x in inputs] # type: ignore\n",
        "        result_data = cls.forward(ctx, *raw_inputs)\n",
        "\n",
        "        requires_grad = any(isinstance(x, Tensor) and x.requires_grad for x in inputs) # type: ignore\n",
        "        out = Tensor(result_data, requires_grad=requires_grad) # type: ignore\n",
        "        if requires_grad:\n",
        "            out.grad_fn = (cls, ctx, inputs)\n",
        "        return out\n",
        "\n",
        "\n",
        "def reduce_grad_to_shape(grad, shape):\n",
        "    \"\"\"\n",
        "    Match the gradient dimension for the input tensor\n",
        "    - grad: output gradient\n",
        "    - shape: original input tensor shape\n",
        "    \"\"\"\n",
        "    # remove expanded dimension for batched data\n",
        "    while grad.ndim > len(shape):\n",
        "        grad = grad.sum(axis=0)\n",
        "\n",
        "    # remove the broadcasted axis\n",
        "    for i, dim in enumerate(shape):\n",
        "        if dim == 1 and (grad.shape[i] != 1):\n",
        "            grad = grad.sum(axis=i, keepdims=True)\n",
        "\n",
        "    return grad\n",
        "\n",
        "class Add(Op):\n",
        "    @staticmethod\n",
        "    def forward(ctx, a, b):\n",
        "        ctx.save_for_backward(a.shape, b.shape)\n",
        "        return a + b\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        a_shape, b_shape = ctx.saved_tensors\n",
        "        grad_a = reduce_grad_to_shape(grad_output, a_shape)\n",
        "        grad_b = reduce_grad_to_shape(grad_output, b_shape)\n",
        "        return grad_a, grad_b\n",
        "\n",
        "class Mul(Op):\n",
        "    @staticmethod\n",
        "    def forward(ctx, a, b):\n",
        "        ctx.save_for_backward(a, b, a.shape, b.shape)\n",
        "        return a * b\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        a, b, a_shape, b_shape = ctx.saved_tensors\n",
        "        grad_a_tmp = grad_output * b\n",
        "        grad_b_tmp = grad_output * a\n",
        "        grad_a = reduce_grad_to_shape(grad_a_tmp, a_shape)\n",
        "        grad_b = reduce_grad_to_shape(grad_b_tmp, b_shape)\n",
        "        return grad_a, grad_b\n",
        "\n",
        "class MatMul(Op):\n",
        "    @staticmethod\n",
        "    def forward(ctx, a, b):\n",
        "        ctx.save_for_backward(a, b, a.shape, b.shape)\n",
        "        return a @ b\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        a, b, a_shape, b_shape = ctx.saved_tensors\n",
        "        grad_a_tmp = grad_output @ b.T\n",
        "        grad_b_tmp = a.T @ grad_output\n",
        "        grad_a = reduce_grad_to_shape(grad_a_tmp, a_shape)\n",
        "        grad_b = reduce_grad_to_shape(grad_b_tmp, b_shape)\n",
        "        return grad_a, grad_b\n",
        "\n",
        "class Pow(Op):\n",
        "    @staticmethod\n",
        "    def forward(ctx, a, b):\n",
        "        ctx.save_for_backward(a, b, a.shape, b.shape)\n",
        "        return a ** b\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        a, b, a_shape, b_shape = ctx.saved_tensors\n",
        "        grad_a_tmp = grad_output * b * (a ** (b-1))\n",
        "        grad_b_tmp = grad_output * (a**b) * np.log(a)\n",
        "        grad_a = reduce_grad_to_shape(grad_a_tmp, a_shape)\n",
        "        grad_b = reduce_grad_to_shape(grad_b_tmp, b_shape)\n",
        "        return grad_a, grad_b\n",
        "\n",
        "class Sum(Op):\n",
        "    @staticmethod\n",
        "    def forward(ctx, a, axis=None, keepdims=False):\n",
        "        ctx.save_for_backward(a.shape, axis, keepdims)\n",
        "        return a.sum(axis=axis, keepdims=keepdims)\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        a_shape, axis, keepdims = ctx.saved_tensors\n",
        "        if axis is None:\n",
        "            grad_a = np.ones(a_shape, dtype=grad_output.dtype) * grad_output\n",
        "        else:\n",
        "            if not keepdims:\n",
        "                grad_output = np.expand_dims(grad_output, axis=axis)\n",
        "            grad_a = np.ones(a_shape, dtype=grad_output.dtype) * grad_output\n",
        "        return grad_a,\n",
        "\n",
        "class ReLU(Op):\n",
        "    @staticmethod\n",
        "    def forward(ctx, a):\n",
        "        mask = a > 0\n",
        "        ctx.save_for_backward(mask)\n",
        "        return np.where(mask, a, 0.0)\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        (mask,) = ctx.saved_tensors\n",
        "        return grad_output * mask\n",
        "\n",
        "class CrossEntropyLoss(Op):\n",
        "    @staticmethod\n",
        "    def forward(ctx, logits, targets):\n",
        "        ctx.save_for_backward(logits, targets)\n",
        "        max_logits = np.max(logits, axis = 1, keepdims = True)\n",
        "        log_softmax = logits - np.log(np.sum(np.exp(logits - max_logits), axis = 1, keepdims = True)) - max_logits\n",
        "        loss = -np.sum(targets * log_softmax, axis=1)\n",
        "        return np.mean(loss)\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        logits, targets = ctx.saved_tensors\n",
        "        exps = np.exp(logits - np.max(logits, axis=1, keepdims = True))\n",
        "        softmax = exps / np.sum(exps, axis=1, keepdims = True)\n",
        "        grad_logits = (softmax - targets) / logits.shape[0]\n",
        "        grad_logits *= grad_output\n",
        "        return grad_logits, None\n",
        "\n",
        "class Log(Op):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x):\n",
        "        ctx.save_for_backward(x)\n",
        "        return np.log(x)\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        (x,) = ctx.saved_tensors\n",
        "        grad_a = grad_output / x\n",
        "        return grad_a,\n",
        "\n",
        "class NLLLoss(Op):\n",
        "    @staticmethod\n",
        "    def forward(ctx, log_probs, targets_one_hot):\n",
        "        loss = -np.sum(targets_one_hot * log_probs, axis=-1)\n",
        "        batch = log_probs.shape[0]\n",
        "        ctx.save_for_backward(targets_one_hot, batch)\n",
        "        return np.mean(loss)\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        targets_one_hot, batch = ctx.saved_tensors\n",
        "        grad_log_probs = -(targets_one_hot/batch) * grad_output\n",
        "        return grad_log_probs, None\n",
        "\n",
        "class Softmax(Op):\n",
        "    @staticmethod\n",
        "    def forward(ctx, logits):\n",
        "        shifted = logits - np.max(logits, axis=-1, keepdims=True)\n",
        "        exp_logits = np.exp(shifted)\n",
        "        probs = exp_logits / np.sum(exp_logits, axis=-1, keepdims=True)\n",
        "        ctx.save_for_backward(probs)\n",
        "        return probs\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        (probs,) = ctx.saved_tensors\n",
        "        grad_a = (grad_output - np.sum(grad_output * probs, axis=-1, keepdims=True)) * probs\n",
        "        return grad_a,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "625bfb31-eebe-4060-9a05-fa2df6bfa2b8",
      "metadata": {
        "id": "625bfb31-eebe-4060-9a05-fa2df6bfa2b8"
      },
      "outputs": [],
      "source": [
        "class Tensor:\n",
        "    def __init__(self, data, requires_grad=False):\n",
        "        if isinstance(data, Tensor):\n",
        "            data = data.data  # unwrap\n",
        "        self.data = np.array(data, dtype=np.float32)\n",
        "        self.requires_grad = requires_grad\n",
        "        self.grad = None\n",
        "        self.grad_fn = None  # (OpClass, ctx, inputs) for non-leaf tensors\n",
        "\n",
        "    # Operator overloads delegate to our Op classes via apply:\n",
        "    def __add__(self, other):      return Add.apply(self, other)\n",
        "    def __radd__(self, other):     return Add.apply(other, self)\n",
        "    def __sub__(self, other):      return Add.apply(self, Mul.apply(other, Tensor(-1.0)))\n",
        "    def __rsub__(self, other):     return Add.apply(other, Mul.apply(self, Tensor(-1.0)))\n",
        "    def __mul__(self, other):      return Mul.apply(self, other)\n",
        "    def __rmul__(self, other):     return Mul.apply(other, self)\n",
        "    def __matmul__(self, other):   return MatMul.apply(self, other)   # matrix @\n",
        "    def __neg__(self):             return Mul.apply(self, Tensor(-1.0))\n",
        "    def __truediv__(self, other):  return Mul.apply(self, Tensor(1.0) / other)\n",
        "    def __pow__(self, exponent):   return Pow.apply(self, exponent)\n",
        "    def __rpow__(self, base):      return Pow.apply(base, self)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"Tensor(data={self.data}, grad={self.grad}, requires_grad={self.requires_grad})\"\n",
        "\n",
        "    # initialization\n",
        "    @staticmethod\n",
        "    def zeros(shape, requires_grad=False):\n",
        "        return Tensor(np.zeros(shape, dtype=np.float32), requires_grad=requires_grad)\n",
        "    @staticmethod\n",
        "    def ones(shape, requires_grad=False):\n",
        "        return Tensor(np.ones(shape, dtype=np.float32), requires_grad=requires_grad)\n",
        "\n",
        "    def backward(self, grad_output=None):\n",
        "        if not self.requires_grad:\n",
        "            raise RuntimeError(\"Cannot call backward on a tensor that does not require grad.\")\n",
        "        # If no grad specified, tensor must be scalar\n",
        "        if grad_output is None:\n",
        "            if self.data.size != 1:\n",
        "                raise RuntimeError(\"grad_output must be provided for non-scalar tensors\")\n",
        "            grad_output = np.ones_like(self.data, dtype=np.float32)\n",
        "        else:\n",
        "            grad_output = np.array(grad_output, dtype=np.float32)\n",
        "        # Initialize this tensor's gradient\n",
        "        self.grad = grad_output\n",
        "\n",
        "        # Build a topologically sorted list of tensors (post-order DFS)\n",
        "        topo_order = []\n",
        "        visited = set()\n",
        "        def build_graph(t):\n",
        "            if isinstance(t, Tensor) and t not in visited:\n",
        "                visited.add(t)\n",
        "                if t.grad_fn is not None:  # not a leaf\n",
        "                    op_cls, ctx, inputs = t.grad_fn\n",
        "                    for inp in inputs:\n",
        "                        build_graph(inp)\n",
        "                topo_order.append(t)\n",
        "        build_graph(self)\n",
        "\n",
        "        # Traverse graph in reverse topological order, apply chain rule\n",
        "        for t in reversed(topo_order):\n",
        "            if t.grad_fn is None:\n",
        "                continue  # leaf node (no backward op)\n",
        "            op_cls, ctx, inputs = t.grad_fn\n",
        "            grad_out = t.grad  # gradient of the output w.rt. this tensor\n",
        "            # Compute gradients of inputs via this op's backward\n",
        "            grad_inputs = op_cls.backward(ctx, grad_out)\n",
        "            if grad_inputs is None:\n",
        "                grad_inputs = ()\n",
        "            elif not isinstance(grad_inputs, tuple):\n",
        "                grad_inputs = (grad_inputs,)\n",
        "            # Accumulate gradients into input tensors\n",
        "            for inp, grad in zip(inputs, grad_inputs):\n",
        "                if isinstance(inp, Tensor) and inp.requires_grad and grad is not None:\n",
        "                    grad = np.array(grad, dtype=np.float32)  # ensure numpy\n",
        "                    if inp.grad is None:\n",
        "                        inp.grad = grad\n",
        "                    else:\n",
        "                        inp.grad += grad\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "b23dd492-74d7-4765-af1c-b64f031e0775",
      "metadata": {
        "id": "b23dd492-74d7-4765-af1c-b64f031e0775",
        "outputId": "8fb5dae4-de13-4563-e6c3-32d4017fb67b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Autograd Graph (parents -> node) ===\n",
            "[694f0] Leaf(shape=(), requires_grad=True)\n",
            "[69af0] Leaf(shape=(), requires_grad=True)\n",
            "[694f0] Leaf(shape=(), requires_grad=True) --Mul--> [6b710] Mul(out_shape=())\n",
            "[69af0] Leaf(shape=(), requires_grad=True) --Mul--> [6b710] Mul(out_shape=())\n",
            "[68200] Leaf(shape=(), requires_grad=True)\n",
            "[6b710] Mul(out_shape=()) --Add--> [91df0] Add(out_shape=())\n",
            "[68200] Leaf(shape=(), requires_grad=True) --Add--> [91df0] Add(out_shape=())\n",
            "[91df0] Add(out_shape=()) --ReLU--> [69fd0] ReLU(out_shape=())\n",
            "\n",
            "=== Backward visit order (reverse topological) ===\n",
            "[69fd0] ReLU(out_shape=())\n",
            "[91df0] Add(out_shape=())\n",
            "[68200] Leaf(shape=(), requires_grad=True)\n",
            "[6b710] Mul(out_shape=())\n",
            "[69af0] Leaf(shape=(), requires_grad=True)\n",
            "[694f0] Leaf(shape=(), requires_grad=True)\n",
            "\n",
            "=== Gradients ===\n",
            "dl/dx = 3.0\n",
            "dl/dw = 2.0\n",
            "dl/db = 1.0\n"
          ]
        }
      ],
      "source": [
        "def _node_name(t: \"Tensor\"):\n",
        "    if t.grad_fn is None:\n",
        "        return f\"Leaf(shape={tuple(t.data.shape)}, requires_grad={t.requires_grad})\"\n",
        "    op_cls, _, _ = t.grad_fn\n",
        "    return f\"{op_cls.__name__}(out_shape={tuple(t.data.shape)})\"\n",
        "\n",
        "def _short_id(obj):\n",
        "    return hex(id(obj))[-5:]  # 보기 좋은 짧은 id\n",
        "\n",
        "def trace_graph(root: \"Tensor\"):\n",
        "    \"\"\"루트 텐서에서 시작해 모든 조상(부모 텐서)들을 수집하고\n",
        "    토폴로지 순서를 반환합니다.\"\"\"\n",
        "    visited = set()\n",
        "    topo = []\n",
        "\n",
        "    def build(t):\n",
        "        if not isinstance(t, Tensor) or t in visited:\n",
        "            return\n",
        "        visited.add(t)\n",
        "        if t.grad_fn is not None:\n",
        "            op_cls, ctx, inputs = t.grad_fn\n",
        "            for inp in inputs:\n",
        "                if isinstance(inp, Tensor):\n",
        "                    build(inp)\n",
        "        topo.append(t)\n",
        "\n",
        "    build(root)\n",
        "    return topo  # 부모가 먼저, root가 마지막(포스트오더)\n",
        "\n",
        "def print_autograd_graph(root: \"Tensor\"):\n",
        "    topo = trace_graph(root)\n",
        "    print(\"=== Autograd Graph (parents -> node) ===\")\n",
        "    for node in topo:\n",
        "        label_node = _node_name(node)\n",
        "        nid = _short_id(node)\n",
        "        if node.grad_fn is None:\n",
        "            print(f\"[{nid}] {label_node}\")\n",
        "            continue\n",
        "        op_cls, _, inputs = node.grad_fn\n",
        "        # 부모들 나열 (부모) --op--> (자식=node)\n",
        "        for inp in inputs:\n",
        "            if isinstance(inp, Tensor):\n",
        "                pid = _short_id(inp)\n",
        "                print(f\"[{pid}] {_node_name(inp)} --{op_cls.__name__}--> [{nid}] {label_node}\")\n",
        "            else:\n",
        "                print(f\"[const {inp}] --{op_cls.__name__}--> [{nid}] {label_node}\")\n",
        "\n",
        "    # backward 방문 순서(실제로 backward에서 사용하는 역토폴로지)\n",
        "    print(\"\\n=== Backward visit order (reverse topological) ===\")\n",
        "    for node in reversed(topo):\n",
        "        print(f\"[{_short_id(node)}] {_node_name(node)}\")\n",
        "\n",
        "\n",
        "# f(x, w, b) = ReLU(x*w + b)\n",
        "x = Tensor(2.0, requires_grad=True)\n",
        "w = Tensor(3.0, requires_grad=True)\n",
        "b = Tensor(-4.0, requires_grad=True)\n",
        "\n",
        "z = x * w + b              # Add(Mul(x,w), b)\n",
        "y = ReLU.apply(z)          # ReLU(z)\n",
        "loss = y                   # 스칼라처럼 취급 (여기선 y가 스칼라)\n",
        "\n",
        "print_autograd_graph(loss)\n",
        "loss.backward()\n",
        "\n",
        "print(\"\\n=== Gradients ===\")\n",
        "print(\"dl/dx =\", x.grad)   # ReLU'(z) * w  (z>0이면 w, 아니면 0)\n",
        "print(\"dl/dw =\", w.grad)   # ReLU'(z) * x\n",
        "print(\"dl/db =\", b.grad)   # ReLU'(z) * 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "13d469f0-5a90-4ffa-99c7-127955807281",
      "metadata": {
        "id": "13d469f0-5a90-4ffa-99c7-127955807281",
        "outputId": "084167c0-f2ee-4770-a357-f7de577a27f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Autograd Graph (parents -> node) ===\n",
            "[6a270] Leaf(shape=(), requires_grad=True)\n",
            "[6a540] Leaf(shape=(), requires_grad=True)\n",
            "[6a270] Leaf(shape=(), requires_grad=True) --Mul--> [6a240] Mul(out_shape=())\n",
            "[6a540] Leaf(shape=(), requires_grad=True) --Mul--> [6a240] Mul(out_shape=())\n",
            "[6a330] Leaf(shape=(), requires_grad=True)\n",
            "[6a240] Mul(out_shape=()) --Add--> [6a420] Add(out_shape=())\n",
            "[6a330] Leaf(shape=(), requires_grad=True) --Add--> [6a420] Add(out_shape=())\n",
            "[6a420] Add(out_shape=()) --ReLU--> [6a510] ReLU(out_shape=())\n",
            "\n",
            "=== Backward visit order (reverse topological) ===\n",
            "[6a510] ReLU(out_shape=())\n",
            "[6a420] Add(out_shape=())\n",
            "[6a330] Leaf(shape=(), requires_grad=True)\n",
            "[6a240] Mul(out_shape=())\n",
            "[6a540] Leaf(shape=(), requires_grad=True)\n",
            "[6a270] Leaf(shape=(), requires_grad=True)\n",
            "\n",
            "=== Gradients ===\n",
            "dl/dx = 3.0\n",
            "dl/dw = 2.0\n",
            "dl/db = 1.0\n"
          ]
        }
      ],
      "source": [
        "x = Tensor(2.0, requires_grad=True)\n",
        "w = Tensor(3.0, requires_grad=True)\n",
        "b = Tensor(-4.0, requires_grad=True)\n",
        "\n",
        "z = x * w + b              # Add(Mul(x,w), b)\n",
        "y = ReLU.apply(z)          # ReLU(z)\n",
        "loss = y                   # 스칼라처럼 취급 (여기선 y가 스칼라)\n",
        "\n",
        "print_autograd_graph(loss)\n",
        "loss.backward()\n",
        "\n",
        "print(\"\\n=== Gradients ===\")\n",
        "print(\"dl/dx =\", x.grad)   # ReLU'(z) * w  (z>0이면 w, 아니면 0)\n",
        "print(\"dl/dw =\", w.grad)   # ReLU'(z) * x\n",
        "print(\"dl/db =\", b.grad)   # ReLU'(z) * 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "601e4d2b-ca5f-4811-82ba-a37e00555854",
      "metadata": {
        "id": "601e4d2b-ca5f-4811-82ba-a37e00555854",
        "outputId": "95cd80a7-bffe-46a3-8f57-789e5be45dcf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Autograd Graph (parents -> node) ===\n",
            "[6a600] Leaf(shape=(3,), requires_grad=True)\n",
            "[6a750] Leaf(shape=(), requires_grad=False)\n",
            "[6a600] Leaf(shape=(3,), requires_grad=True) --Mul--> [6a870] Mul(out_shape=(3,))\n",
            "[6a750] Leaf(shape=(), requires_grad=False) --Mul--> [6a870] Mul(out_shape=(3,))\n",
            "[6a8a0] Leaf(shape=(), requires_grad=False)\n",
            "[6a870] Mul(out_shape=(3,)) --Add--> [6a6c0] Add(out_shape=(3,))\n",
            "[6a8a0] Leaf(shape=(), requires_grad=False) --Add--> [6a6c0] Add(out_shape=(3,))\n",
            "[6a6c0] Add(out_shape=(3,)) --ReLU--> [6a8d0] ReLU(out_shape=(3,))\n",
            "[6a8d0] ReLU(out_shape=(3,)) --Sum--> [6a960] Sum(out_shape=())\n",
            "\n",
            "=== Backward visit order (reverse topological) ===\n",
            "[6a960] Sum(out_shape=())\n",
            "[6a8d0] ReLU(out_shape=(3,))\n",
            "[6a6c0] Add(out_shape=(3,))\n",
            "[6a8a0] Leaf(shape=(), requires_grad=False)\n",
            "[6a870] Mul(out_shape=(3,))\n",
            "[6a750] Leaf(shape=(), requires_grad=False)\n",
            "[6a600] Leaf(shape=(3,), requires_grad=True)\n",
            "\n",
            "=== Values & Gradients ===\n",
            "y = [0. 1. 0.]\n",
            "dl/da = [0. 2. 0.]\n"
          ]
        }
      ],
      "source": [
        "a = Tensor(np.array([0.4, 1.0, -2.0], dtype=np.float32), requires_grad=True)\n",
        "\n",
        "y = ReLU.apply(a * Tensor(2.0) - Tensor(1.0))   # elementwise\n",
        "loss = Sum.apply(y)                             # Sum op로 스칼라화\n",
        "\n",
        "print_autograd_graph(loss)\n",
        "loss.backward()\n",
        "\n",
        "print(\"\\n=== Values & Gradients ===\")\n",
        "print(\"y =\", y.data)        # [0., 1., 0.]  (ReLU 결과)\n",
        "print(\"dl/da =\", a.grad)    # ReLU>0인 위치에서 2, 나머지는 0  → [0., 2., 0.]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "d7eecd87-5df5-40dd-93f1-f909d28871bd",
      "metadata": {
        "id": "d7eecd87-5df5-40dd-93f1-f909d28871bd",
        "outputId": "040409a1-a4ef-410f-d342-a1bc1a12b853",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded train-images-idx3-ubyte.gz\n",
            "Downloaded train-labels-idx1-ubyte.gz\n",
            "Downloaded t10k-images-idx3-ubyte.gz\n",
            "Downloaded t10k-labels-idx1-ubyte.gz\n",
            "MNIST data loaded: (60000, 784) (60000, 10)\n"
          ]
        }
      ],
      "source": [
        "import os, gzip\n",
        "from urllib.request import urlretrieve\n",
        "\n",
        "def load_mnist(path=None):\n",
        "    \"\"\"Download MNIST and load it into NumPy arrays.\"\"\"\n",
        "    #url_base = \"http://yann.lecun.com/exdb/mnist/\"\n",
        "    url_base = \"https://ossci-datasets.s3.amazonaws.com/mnist/\"\n",
        "\n",
        "    files = {\n",
        "        \"train_images\": \"train-images-idx3-ubyte.gz\",\n",
        "        \"train_labels\": \"train-labels-idx1-ubyte.gz\",\n",
        "        \"test_images\":  \"t10k-images-idx3-ubyte.gz\",\n",
        "        \"test_labels\":  \"t10k-labels-idx1-ubyte.gz\"\n",
        "    }\n",
        "    # Default path to ./data/mnist\n",
        "    if path is None:\n",
        "        path = os.path.join(os.path.expanduser(\"./\"), \"data\", \"mnist\")\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "\n",
        "    # Download missing files\n",
        "    for name, filename in files.items():\n",
        "        filepath = os.path.join(path, filename)\n",
        "        if not os.path.isfile(filepath):\n",
        "            urlretrieve(url_base + filename, filepath)\n",
        "            print(f\"Downloaded {filename}\")\n",
        "\n",
        "    # Load images\n",
        "    def _read_images(filename):\n",
        "        with gzip.open(os.path.join(path, filename), \"rb\") as f:\n",
        "            data = f.read()\n",
        "            # The first 16 bytes are header (magic, num, rows, cols)\n",
        "            images = np.frombuffer(data, dtype=np.uint8, offset=16)\n",
        "        images = images.reshape(-1, 28*28).astype(np.float32) / 255.0\n",
        "        return images\n",
        "\n",
        "    # Load labels\n",
        "    def _read_labels(filename):\n",
        "        with gzip.open(os.path.join(path, filename), \"rb\") as f:\n",
        "            data = f.read()\n",
        "            # First 8 bytes are header (magic, num)\n",
        "            labels = np.frombuffer(data, dtype=np.uint8, offset=8)\n",
        "        # Convert to one-hot vectors of length 10\n",
        "        one_hot = np.zeros((labels.size, 10), dtype=np.float32)\n",
        "        one_hot[np.arange(labels.size), labels] = 1.0\n",
        "        return one_hot\n",
        "\n",
        "    # Read all parts\n",
        "    X_train = _read_images(files[\"train_images\"])\n",
        "    y_train = _read_labels(files[\"train_labels\"])\n",
        "    X_test  = _read_images(files[\"test_images\"])\n",
        "    y_test  = _read_labels(files[\"test_labels\"])\n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "# Load the MNIST data (this will download if not already present)\n",
        "X_train, y_train, X_test, y_test = load_mnist()\n",
        "print(\"MNIST data loaded:\", X_train.shape, y_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "dab29554-1619-432e-87e5-7919b4e1de98",
      "metadata": {
        "id": "dab29554-1619-432e-87e5-7919b4e1de98",
        "outputId": "0d040f26-3c9d-442f-d5b2-77e0fa0f2683",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Itr 0 / 60000: avg loss = 0.003838\n",
            "Epoch 1, Itr 1000 / 60000: avg loss = 0.000042\n",
            "Epoch 1, Itr 2000 / 60000: avg loss = 0.000040\n",
            "Epoch 1, Itr 3000 / 60000: avg loss = 0.000040\n",
            "Epoch 1, Itr 4000 / 60000: avg loss = 0.000039\n",
            "Epoch 1, Itr 5000 / 60000: avg loss = 0.000039\n",
            "Epoch 1, Itr 6000 / 60000: avg loss = 0.000039\n",
            "Epoch 1, Itr 7000 / 60000: avg loss = 0.000039\n",
            "Epoch 1, Itr 8000 / 60000: avg loss = 0.000039\n",
            "Epoch 1, Itr 9000 / 60000: avg loss = 0.000039\n",
            "Epoch 1, Itr 10000 / 60000: avg loss = 0.000039\n",
            "Epoch 1, Itr 11000 / 60000: avg loss = 0.000039\n",
            "Epoch 1, Itr 12000 / 60000: avg loss = 0.000039\n",
            "Epoch 1, Itr 13000 / 60000: avg loss = 0.000039\n",
            "Epoch 1, Itr 14000 / 60000: avg loss = 0.000039\n",
            "Epoch 1, Itr 15000 / 60000: avg loss = 0.000039\n",
            "Epoch 1, Itr 16000 / 60000: avg loss = 0.000039\n",
            "Epoch 1, Itr 17000 / 60000: avg loss = 0.000039\n",
            "Epoch 1, Itr 18000 / 60000: avg loss = 0.000039\n",
            "Epoch 1, Itr 19000 / 60000: avg loss = 0.000039\n",
            "Epoch 1, Itr 20000 / 60000: avg loss = 0.000039\n",
            "Epoch 1, Itr 21000 / 60000: avg loss = 0.000039\n",
            "Epoch 1, Itr 22000 / 60000: avg loss = 0.000038\n",
            "Epoch 1, Itr 23000 / 60000: avg loss = 0.000038\n",
            "Epoch 1, Itr 24000 / 60000: avg loss = 0.000038\n",
            "Epoch 1, Itr 25000 / 60000: avg loss = 0.000038\n",
            "Epoch 1, Itr 26000 / 60000: avg loss = 0.000038\n",
            "Epoch 1, Itr 27000 / 60000: avg loss = 0.000038\n",
            "Epoch 1, Itr 28000 / 60000: avg loss = 0.000038\n",
            "Epoch 1, Itr 29000 / 60000: avg loss = 0.000038\n",
            "Epoch 1, Itr 30000 / 60000: avg loss = 0.000038\n",
            "Epoch 1, Itr 31000 / 60000: avg loss = 0.000038\n",
            "Epoch 1, Itr 32000 / 60000: avg loss = 0.000038\n",
            "Epoch 1, Itr 33000 / 60000: avg loss = 0.000037\n",
            "Epoch 1, Itr 34000 / 60000: avg loss = 0.000037\n",
            "Epoch 1, Itr 35000 / 60000: avg loss = 0.000037\n",
            "Epoch 1, Itr 36000 / 60000: avg loss = 0.000037\n",
            "Epoch 1, Itr 37000 / 60000: avg loss = 0.000036\n",
            "Epoch 1, Itr 38000 / 60000: avg loss = 0.000036\n",
            "Epoch 1, Itr 39000 / 60000: avg loss = 0.000036\n",
            "Epoch 1, Itr 40000 / 60000: avg loss = 0.000035\n",
            "Epoch 1, Itr 41000 / 60000: avg loss = 0.000035\n",
            "Epoch 1, Itr 42000 / 60000: avg loss = 0.000034\n",
            "Epoch 1, Itr 43000 / 60000: avg loss = 0.000034\n",
            "Epoch 1, Itr 44000 / 60000: avg loss = 0.000034\n",
            "Epoch 1, Itr 45000 / 60000: avg loss = 0.000033\n",
            "Epoch 1, Itr 46000 / 60000: avg loss = 0.000033\n",
            "Epoch 1, Itr 47000 / 60000: avg loss = 0.000032\n",
            "Epoch 1, Itr 48000 / 60000: avg loss = 0.000032\n",
            "Epoch 1, Itr 49000 / 60000: avg loss = 0.000031\n",
            "Epoch 1, Itr 50000 / 60000: avg loss = 0.000031\n",
            "Epoch 1, Itr 51000 / 60000: avg loss = 0.000031\n",
            "Epoch 1, Itr 52000 / 60000: avg loss = 0.000030\n",
            "Epoch 1, Itr 53000 / 60000: avg loss = 0.000030\n",
            "Epoch 1, Itr 54000 / 60000: avg loss = 0.000030\n",
            "Epoch 1, Itr 55000 / 60000: avg loss = 0.000029\n",
            "Epoch 1, Itr 56000 / 60000: avg loss = 0.000029\n",
            "Epoch 1, Itr 57000 / 60000: avg loss = 0.000029\n",
            "Epoch 1, Itr 58000 / 60000: avg loss = 0.000028\n",
            "Epoch 1, Itr 59000 / 60000: avg loss = 0.000028\n",
            "Epoch 1 / Test accuracy: 80.75%\n",
            "Epoch 2, Itr 0 / 60000: avg loss = 0.000881\n",
            "Epoch 2, Itr 1000 / 60000: avg loss = 0.000011\n",
            "Epoch 2, Itr 2000 / 60000: avg loss = 0.000011\n",
            "Epoch 2, Itr 3000 / 60000: avg loss = 0.000011\n",
            "Epoch 2, Itr 4000 / 60000: avg loss = 0.000011\n",
            "Epoch 2, Itr 5000 / 60000: avg loss = 0.000010\n",
            "Epoch 2, Itr 6000 / 60000: avg loss = 0.000011\n",
            "Epoch 2, Itr 7000 / 60000: avg loss = 0.000010\n",
            "Epoch 2, Itr 8000 / 60000: avg loss = 0.000010\n",
            "Epoch 2, Itr 9000 / 60000: avg loss = 0.000010\n",
            "Epoch 2, Itr 10000 / 60000: avg loss = 0.000010\n",
            "Epoch 2, Itr 11000 / 60000: avg loss = 0.000010\n",
            "Epoch 2, Itr 12000 / 60000: avg loss = 0.000010\n",
            "Epoch 2, Itr 13000 / 60000: avg loss = 0.000010\n",
            "Epoch 2, Itr 14000 / 60000: avg loss = 0.000010\n",
            "Epoch 2, Itr 15000 / 60000: avg loss = 0.000010\n",
            "Epoch 2, Itr 16000 / 60000: avg loss = 0.000010\n",
            "Epoch 2, Itr 17000 / 60000: avg loss = 0.000010\n",
            "Epoch 2, Itr 18000 / 60000: avg loss = 0.000009\n",
            "Epoch 2, Itr 19000 / 60000: avg loss = 0.000009\n",
            "Epoch 2, Itr 20000 / 60000: avg loss = 0.000009\n",
            "Epoch 2, Itr 21000 / 60000: avg loss = 0.000009\n",
            "Epoch 2, Itr 22000 / 60000: avg loss = 0.000009\n",
            "Epoch 2, Itr 23000 / 60000: avg loss = 0.000009\n",
            "Epoch 2, Itr 24000 / 60000: avg loss = 0.000009\n",
            "Epoch 2, Itr 25000 / 60000: avg loss = 0.000009\n",
            "Epoch 2, Itr 26000 / 60000: avg loss = 0.000009\n",
            "Epoch 2, Itr 27000 / 60000: avg loss = 0.000009\n",
            "Epoch 2, Itr 28000 / 60000: avg loss = 0.000009\n",
            "Epoch 2, Itr 29000 / 60000: avg loss = 0.000009\n",
            "Epoch 2, Itr 30000 / 60000: avg loss = 0.000009\n",
            "Epoch 2, Itr 31000 / 60000: avg loss = 0.000009\n",
            "Epoch 2, Itr 32000 / 60000: avg loss = 0.000009\n",
            "Epoch 2, Itr 33000 / 60000: avg loss = 0.000009\n",
            "Epoch 2, Itr 34000 / 60000: avg loss = 0.000009\n",
            "Epoch 2, Itr 35000 / 60000: avg loss = 0.000009\n",
            "Epoch 2, Itr 36000 / 60000: avg loss = 0.000009\n",
            "Epoch 2, Itr 37000 / 60000: avg loss = 0.000009\n",
            "Epoch 2, Itr 38000 / 60000: avg loss = 0.000009\n",
            "Epoch 2, Itr 39000 / 60000: avg loss = 0.000008\n",
            "Epoch 2, Itr 40000 / 60000: avg loss = 0.000008\n",
            "Epoch 2, Itr 41000 / 60000: avg loss = 0.000008\n",
            "Epoch 2, Itr 42000 / 60000: avg loss = 0.000008\n",
            "Epoch 2, Itr 43000 / 60000: avg loss = 0.000008\n",
            "Epoch 2, Itr 44000 / 60000: avg loss = 0.000008\n",
            "Epoch 2, Itr 45000 / 60000: avg loss = 0.000008\n",
            "Epoch 2, Itr 46000 / 60000: avg loss = 0.000008\n",
            "Epoch 2, Itr 47000 / 60000: avg loss = 0.000008\n",
            "Epoch 2, Itr 48000 / 60000: avg loss = 0.000008\n",
            "Epoch 2, Itr 49000 / 60000: avg loss = 0.000008\n",
            "Epoch 2, Itr 50000 / 60000: avg loss = 0.000008\n",
            "Epoch 2, Itr 51000 / 60000: avg loss = 0.000008\n",
            "Epoch 2, Itr 52000 / 60000: avg loss = 0.000008\n",
            "Epoch 2, Itr 53000 / 60000: avg loss = 0.000008\n",
            "Epoch 2, Itr 54000 / 60000: avg loss = 0.000008\n",
            "Epoch 2, Itr 55000 / 60000: avg loss = 0.000008\n",
            "Epoch 2, Itr 56000 / 60000: avg loss = 0.000008\n",
            "Epoch 2, Itr 57000 / 60000: avg loss = 0.000008\n",
            "Epoch 2, Itr 58000 / 60000: avg loss = 0.000008\n",
            "Epoch 2, Itr 59000 / 60000: avg loss = 0.000008\n",
            "Epoch 2 / Test accuracy: 91.33%\n",
            "Epoch 3, Itr 0 / 60000: avg loss = 0.000511\n",
            "Epoch 3, Itr 1000 / 60000: avg loss = 0.000006\n",
            "Epoch 3, Itr 2000 / 60000: avg loss = 0.000005\n",
            "Epoch 3, Itr 3000 / 60000: avg loss = 0.000005\n",
            "Epoch 3, Itr 4000 / 60000: avg loss = 0.000005\n",
            "Epoch 3, Itr 5000 / 60000: avg loss = 0.000005\n",
            "Epoch 3, Itr 6000 / 60000: avg loss = 0.000005\n",
            "Epoch 3, Itr 7000 / 60000: avg loss = 0.000005\n",
            "Epoch 3, Itr 8000 / 60000: avg loss = 0.000005\n",
            "Epoch 3, Itr 9000 / 60000: avg loss = 0.000005\n",
            "Epoch 3, Itr 10000 / 60000: avg loss = 0.000005\n",
            "Epoch 3, Itr 11000 / 60000: avg loss = 0.000005\n",
            "Epoch 3, Itr 12000 / 60000: avg loss = 0.000005\n",
            "Epoch 3, Itr 13000 / 60000: avg loss = 0.000005\n",
            "Epoch 3, Itr 14000 / 60000: avg loss = 0.000005\n",
            "Epoch 3, Itr 15000 / 60000: avg loss = 0.000005\n",
            "Epoch 3, Itr 16000 / 60000: avg loss = 0.000005\n",
            "Epoch 3, Itr 17000 / 60000: avg loss = 0.000005\n",
            "Epoch 3, Itr 18000 / 60000: avg loss = 0.000005\n",
            "Epoch 3, Itr 19000 / 60000: avg loss = 0.000005\n",
            "Epoch 3, Itr 20000 / 60000: avg loss = 0.000005\n",
            "Epoch 3, Itr 21000 / 60000: avg loss = 0.000005\n",
            "Epoch 3, Itr 22000 / 60000: avg loss = 0.000005\n",
            "Epoch 3, Itr 23000 / 60000: avg loss = 0.000005\n",
            "Epoch 3, Itr 24000 / 60000: avg loss = 0.000005\n",
            "Epoch 3, Itr 25000 / 60000: avg loss = 0.000005\n",
            "Epoch 3, Itr 26000 / 60000: avg loss = 0.000005\n",
            "Epoch 3, Itr 27000 / 60000: avg loss = 0.000005\n",
            "Epoch 3, Itr 28000 / 60000: avg loss = 0.000005\n",
            "Epoch 3, Itr 29000 / 60000: avg loss = 0.000005\n",
            "Epoch 3, Itr 30000 / 60000: avg loss = 0.000005\n",
            "Epoch 3, Itr 31000 / 60000: avg loss = 0.000005\n",
            "Epoch 3, Itr 32000 / 60000: avg loss = 0.000005\n",
            "Epoch 3, Itr 33000 / 60000: avg loss = 0.000005\n",
            "Epoch 3, Itr 34000 / 60000: avg loss = 0.000005\n",
            "Epoch 3, Itr 35000 / 60000: avg loss = 0.000005\n",
            "Epoch 3, Itr 36000 / 60000: avg loss = 0.000005\n",
            "Epoch 3, Itr 37000 / 60000: avg loss = 0.000005\n",
            "Epoch 3, Itr 38000 / 60000: avg loss = 0.000005\n",
            "Epoch 3, Itr 39000 / 60000: avg loss = 0.000005\n",
            "Epoch 3, Itr 40000 / 60000: avg loss = 0.000005\n",
            "Epoch 3, Itr 41000 / 60000: avg loss = 0.000005\n",
            "Epoch 3, Itr 42000 / 60000: avg loss = 0.000005\n",
            "Epoch 3, Itr 43000 / 60000: avg loss = 0.000005\n",
            "Epoch 3, Itr 44000 / 60000: avg loss = 0.000005\n",
            "Epoch 3, Itr 45000 / 60000: avg loss = 0.000005\n",
            "Epoch 3, Itr 46000 / 60000: avg loss = 0.000005\n",
            "Epoch 3, Itr 47000 / 60000: avg loss = 0.000005\n",
            "Epoch 3, Itr 48000 / 60000: avg loss = 0.000005\n",
            "Epoch 3, Itr 49000 / 60000: avg loss = 0.000005\n",
            "Epoch 3, Itr 50000 / 60000: avg loss = 0.000005\n",
            "Epoch 3, Itr 51000 / 60000: avg loss = 0.000005\n",
            "Epoch 3, Itr 52000 / 60000: avg loss = 0.000005\n",
            "Epoch 3, Itr 53000 / 60000: avg loss = 0.000005\n",
            "Epoch 3, Itr 54000 / 60000: avg loss = 0.000005\n",
            "Epoch 3, Itr 55000 / 60000: avg loss = 0.000005\n",
            "Epoch 3, Itr 56000 / 60000: avg loss = 0.000005\n",
            "Epoch 3, Itr 57000 / 60000: avg loss = 0.000005\n",
            "Epoch 3, Itr 58000 / 60000: avg loss = 0.000005\n",
            "Epoch 3, Itr 59000 / 60000: avg loss = 0.000005\n",
            "Epoch 3 / Test accuracy: 93.77%\n",
            "Epoch 4, Itr 0 / 60000: avg loss = 0.000366\n",
            "Epoch 4, Itr 1000 / 60000: avg loss = 0.000004\n",
            "Epoch 4, Itr 2000 / 60000: avg loss = 0.000004\n",
            "Epoch 4, Itr 3000 / 60000: avg loss = 0.000004\n",
            "Epoch 4, Itr 4000 / 60000: avg loss = 0.000004\n",
            "Epoch 4, Itr 5000 / 60000: avg loss = 0.000004\n",
            "Epoch 4, Itr 6000 / 60000: avg loss = 0.000004\n",
            "Epoch 4, Itr 7000 / 60000: avg loss = 0.000004\n",
            "Epoch 4, Itr 8000 / 60000: avg loss = 0.000004\n",
            "Epoch 4, Itr 9000 / 60000: avg loss = 0.000004\n",
            "Epoch 4, Itr 10000 / 60000: avg loss = 0.000004\n",
            "Epoch 4, Itr 11000 / 60000: avg loss = 0.000003\n",
            "Epoch 4, Itr 12000 / 60000: avg loss = 0.000003\n",
            "Epoch 4, Itr 13000 / 60000: avg loss = 0.000003\n",
            "Epoch 4, Itr 14000 / 60000: avg loss = 0.000003\n",
            "Epoch 4, Itr 15000 / 60000: avg loss = 0.000003\n",
            "Epoch 4, Itr 16000 / 60000: avg loss = 0.000003\n",
            "Epoch 4, Itr 17000 / 60000: avg loss = 0.000003\n",
            "Epoch 4, Itr 18000 / 60000: avg loss = 0.000003\n",
            "Epoch 4, Itr 19000 / 60000: avg loss = 0.000003\n",
            "Epoch 4, Itr 20000 / 60000: avg loss = 0.000003\n",
            "Epoch 4, Itr 21000 / 60000: avg loss = 0.000003\n",
            "Epoch 4, Itr 22000 / 60000: avg loss = 0.000003\n",
            "Epoch 4, Itr 23000 / 60000: avg loss = 0.000003\n",
            "Epoch 4, Itr 24000 / 60000: avg loss = 0.000003\n",
            "Epoch 4, Itr 25000 / 60000: avg loss = 0.000003\n",
            "Epoch 4, Itr 26000 / 60000: avg loss = 0.000003\n",
            "Epoch 4, Itr 27000 / 60000: avg loss = 0.000003\n",
            "Epoch 4, Itr 28000 / 60000: avg loss = 0.000003\n",
            "Epoch 4, Itr 29000 / 60000: avg loss = 0.000003\n",
            "Epoch 4, Itr 30000 / 60000: avg loss = 0.000003\n",
            "Epoch 4, Itr 31000 / 60000: avg loss = 0.000003\n",
            "Epoch 4, Itr 32000 / 60000: avg loss = 0.000003\n",
            "Epoch 4, Itr 33000 / 60000: avg loss = 0.000003\n",
            "Epoch 4, Itr 34000 / 60000: avg loss = 0.000003\n",
            "Epoch 4, Itr 35000 / 60000: avg loss = 0.000003\n",
            "Epoch 4, Itr 36000 / 60000: avg loss = 0.000003\n",
            "Epoch 4, Itr 37000 / 60000: avg loss = 0.000003\n",
            "Epoch 4, Itr 38000 / 60000: avg loss = 0.000003\n",
            "Epoch 4, Itr 39000 / 60000: avg loss = 0.000003\n",
            "Epoch 4, Itr 40000 / 60000: avg loss = 0.000003\n",
            "Epoch 4, Itr 41000 / 60000: avg loss = 0.000003\n",
            "Epoch 4, Itr 42000 / 60000: avg loss = 0.000003\n",
            "Epoch 4, Itr 43000 / 60000: avg loss = 0.000003\n",
            "Epoch 4, Itr 44000 / 60000: avg loss = 0.000003\n",
            "Epoch 4, Itr 45000 / 60000: avg loss = 0.000003\n",
            "Epoch 4, Itr 46000 / 60000: avg loss = 0.000003\n",
            "Epoch 4, Itr 47000 / 60000: avg loss = 0.000003\n",
            "Epoch 4, Itr 48000 / 60000: avg loss = 0.000003\n",
            "Epoch 4, Itr 49000 / 60000: avg loss = 0.000003\n",
            "Epoch 4, Itr 50000 / 60000: avg loss = 0.000003\n",
            "Epoch 4, Itr 51000 / 60000: avg loss = 0.000003\n",
            "Epoch 4, Itr 52000 / 60000: avg loss = 0.000003\n",
            "Epoch 4, Itr 53000 / 60000: avg loss = 0.000003\n",
            "Epoch 4, Itr 54000 / 60000: avg loss = 0.000003\n",
            "Epoch 4, Itr 55000 / 60000: avg loss = 0.000003\n",
            "Epoch 4, Itr 56000 / 60000: avg loss = 0.000003\n",
            "Epoch 4, Itr 57000 / 60000: avg loss = 0.000003\n",
            "Epoch 4, Itr 58000 / 60000: avg loss = 0.000003\n",
            "Epoch 4, Itr 59000 / 60000: avg loss = 0.000003\n",
            "Epoch 4 / Test accuracy: 95.01%\n",
            "Epoch 5, Itr 0 / 60000: avg loss = 0.000285\n",
            "Epoch 5, Itr 1000 / 60000: avg loss = 0.000003\n",
            "Epoch 5, Itr 2000 / 60000: avg loss = 0.000003\n",
            "Epoch 5, Itr 3000 / 60000: avg loss = 0.000003\n",
            "Epoch 5, Itr 4000 / 60000: avg loss = 0.000003\n",
            "Epoch 5, Itr 5000 / 60000: avg loss = 0.000003\n",
            "Epoch 5, Itr 6000 / 60000: avg loss = 0.000003\n",
            "Epoch 5, Itr 7000 / 60000: avg loss = 0.000003\n",
            "Epoch 5, Itr 8000 / 60000: avg loss = 0.000003\n",
            "Epoch 5, Itr 9000 / 60000: avg loss = 0.000003\n",
            "Epoch 5, Itr 10000 / 60000: avg loss = 0.000003\n",
            "Epoch 5, Itr 11000 / 60000: avg loss = 0.000003\n",
            "Epoch 5, Itr 12000 / 60000: avg loss = 0.000003\n",
            "Epoch 5, Itr 13000 / 60000: avg loss = 0.000003\n",
            "Epoch 5, Itr 14000 / 60000: avg loss = 0.000003\n",
            "Epoch 5, Itr 15000 / 60000: avg loss = 0.000003\n",
            "Epoch 5, Itr 16000 / 60000: avg loss = 0.000003\n",
            "Epoch 5, Itr 17000 / 60000: avg loss = 0.000003\n",
            "Epoch 5, Itr 18000 / 60000: avg loss = 0.000003\n",
            "Epoch 5, Itr 19000 / 60000: avg loss = 0.000003\n",
            "Epoch 5, Itr 20000 / 60000: avg loss = 0.000003\n",
            "Epoch 5, Itr 21000 / 60000: avg loss = 0.000003\n",
            "Epoch 5, Itr 22000 / 60000: avg loss = 0.000003\n",
            "Epoch 5, Itr 23000 / 60000: avg loss = 0.000003\n",
            "Epoch 5, Itr 24000 / 60000: avg loss = 0.000003\n",
            "Epoch 5, Itr 25000 / 60000: avg loss = 0.000003\n",
            "Epoch 5, Itr 26000 / 60000: avg loss = 0.000003\n",
            "Epoch 5, Itr 27000 / 60000: avg loss = 0.000003\n",
            "Epoch 5, Itr 28000 / 60000: avg loss = 0.000003\n",
            "Epoch 5, Itr 29000 / 60000: avg loss = 0.000003\n",
            "Epoch 5, Itr 30000 / 60000: avg loss = 0.000003\n",
            "Epoch 5, Itr 31000 / 60000: avg loss = 0.000003\n",
            "Epoch 5, Itr 32000 / 60000: avg loss = 0.000003\n",
            "Epoch 5, Itr 33000 / 60000: avg loss = 0.000003\n",
            "Epoch 5, Itr 34000 / 60000: avg loss = 0.000003\n",
            "Epoch 5, Itr 35000 / 60000: avg loss = 0.000003\n",
            "Epoch 5, Itr 36000 / 60000: avg loss = 0.000003\n",
            "Epoch 5, Itr 37000 / 60000: avg loss = 0.000003\n",
            "Epoch 5, Itr 38000 / 60000: avg loss = 0.000003\n",
            "Epoch 5, Itr 39000 / 60000: avg loss = 0.000003\n",
            "Epoch 5, Itr 40000 / 60000: avg loss = 0.000003\n",
            "Epoch 5, Itr 41000 / 60000: avg loss = 0.000003\n",
            "Epoch 5, Itr 42000 / 60000: avg loss = 0.000003\n",
            "Epoch 5, Itr 43000 / 60000: avg loss = 0.000003\n",
            "Epoch 5, Itr 44000 / 60000: avg loss = 0.000003\n",
            "Epoch 5, Itr 45000 / 60000: avg loss = 0.000003\n",
            "Epoch 5, Itr 46000 / 60000: avg loss = 0.000003\n",
            "Epoch 5, Itr 47000 / 60000: avg loss = 0.000003\n",
            "Epoch 5, Itr 48000 / 60000: avg loss = 0.000003\n",
            "Epoch 5, Itr 49000 / 60000: avg loss = 0.000003\n",
            "Epoch 5, Itr 50000 / 60000: avg loss = 0.000003\n",
            "Epoch 5, Itr 51000 / 60000: avg loss = 0.000003\n",
            "Epoch 5, Itr 52000 / 60000: avg loss = 0.000003\n",
            "Epoch 5, Itr 53000 / 60000: avg loss = 0.000003\n",
            "Epoch 5, Itr 54000 / 60000: avg loss = 0.000003\n",
            "Epoch 5, Itr 55000 / 60000: avg loss = 0.000003\n",
            "Epoch 5, Itr 56000 / 60000: avg loss = 0.000003\n",
            "Epoch 5, Itr 57000 / 60000: avg loss = 0.000003\n",
            "Epoch 5, Itr 58000 / 60000: avg loss = 0.000003\n",
            "Epoch 5, Itr 59000 / 60000: avg loss = 0.000003\n",
            "Epoch 5 / Test accuracy: 95.76%\n",
            "Epoch 6, Itr 0 / 60000: avg loss = 0.000309\n",
            "Epoch 6, Itr 1000 / 60000: avg loss = 0.000002\n",
            "Epoch 6, Itr 2000 / 60000: avg loss = 0.000002\n",
            "Epoch 6, Itr 3000 / 60000: avg loss = 0.000002\n",
            "Epoch 6, Itr 4000 / 60000: avg loss = 0.000002\n",
            "Epoch 6, Itr 5000 / 60000: avg loss = 0.000002\n",
            "Epoch 6, Itr 6000 / 60000: avg loss = 0.000002\n",
            "Epoch 6, Itr 7000 / 60000: avg loss = 0.000002\n",
            "Epoch 6, Itr 8000 / 60000: avg loss = 0.000002\n",
            "Epoch 6, Itr 9000 / 60000: avg loss = 0.000002\n",
            "Epoch 6, Itr 10000 / 60000: avg loss = 0.000002\n",
            "Epoch 6, Itr 11000 / 60000: avg loss = 0.000002\n",
            "Epoch 6, Itr 12000 / 60000: avg loss = 0.000002\n",
            "Epoch 6, Itr 13000 / 60000: avg loss = 0.000002\n",
            "Epoch 6, Itr 14000 / 60000: avg loss = 0.000002\n",
            "Epoch 6, Itr 15000 / 60000: avg loss = 0.000002\n",
            "Epoch 6, Itr 16000 / 60000: avg loss = 0.000002\n",
            "Epoch 6, Itr 17000 / 60000: avg loss = 0.000002\n",
            "Epoch 6, Itr 18000 / 60000: avg loss = 0.000002\n",
            "Epoch 6, Itr 19000 / 60000: avg loss = 0.000002\n",
            "Epoch 6, Itr 20000 / 60000: avg loss = 0.000002\n",
            "Epoch 6, Itr 21000 / 60000: avg loss = 0.000002\n",
            "Epoch 6, Itr 22000 / 60000: avg loss = 0.000002\n",
            "Epoch 6, Itr 23000 / 60000: avg loss = 0.000002\n",
            "Epoch 6, Itr 24000 / 60000: avg loss = 0.000002\n",
            "Epoch 6, Itr 25000 / 60000: avg loss = 0.000002\n",
            "Epoch 6, Itr 26000 / 60000: avg loss = 0.000002\n",
            "Epoch 6, Itr 27000 / 60000: avg loss = 0.000002\n",
            "Epoch 6, Itr 28000 / 60000: avg loss = 0.000002\n",
            "Epoch 6, Itr 29000 / 60000: avg loss = 0.000002\n",
            "Epoch 6, Itr 30000 / 60000: avg loss = 0.000002\n",
            "Epoch 6, Itr 31000 / 60000: avg loss = 0.000002\n",
            "Epoch 6, Itr 32000 / 60000: avg loss = 0.000002\n",
            "Epoch 6, Itr 33000 / 60000: avg loss = 0.000002\n",
            "Epoch 6, Itr 34000 / 60000: avg loss = 0.000002\n",
            "Epoch 6, Itr 35000 / 60000: avg loss = 0.000002\n",
            "Epoch 6, Itr 36000 / 60000: avg loss = 0.000002\n",
            "Epoch 6, Itr 37000 / 60000: avg loss = 0.000002\n",
            "Epoch 6, Itr 38000 / 60000: avg loss = 0.000002\n",
            "Epoch 6, Itr 39000 / 60000: avg loss = 0.000002\n",
            "Epoch 6, Itr 40000 / 60000: avg loss = 0.000002\n",
            "Epoch 6, Itr 41000 / 60000: avg loss = 0.000002\n",
            "Epoch 6, Itr 42000 / 60000: avg loss = 0.000002\n",
            "Epoch 6, Itr 43000 / 60000: avg loss = 0.000002\n",
            "Epoch 6, Itr 44000 / 60000: avg loss = 0.000002\n",
            "Epoch 6, Itr 45000 / 60000: avg loss = 0.000002\n",
            "Epoch 6, Itr 46000 / 60000: avg loss = 0.000002\n",
            "Epoch 6, Itr 47000 / 60000: avg loss = 0.000002\n",
            "Epoch 6, Itr 48000 / 60000: avg loss = 0.000002\n",
            "Epoch 6, Itr 49000 / 60000: avg loss = 0.000002\n",
            "Epoch 6, Itr 50000 / 60000: avg loss = 0.000002\n",
            "Epoch 6, Itr 51000 / 60000: avg loss = 0.000002\n",
            "Epoch 6, Itr 52000 / 60000: avg loss = 0.000002\n",
            "Epoch 6, Itr 53000 / 60000: avg loss = 0.000002\n",
            "Epoch 6, Itr 54000 / 60000: avg loss = 0.000002\n",
            "Epoch 6, Itr 55000 / 60000: avg loss = 0.000002\n",
            "Epoch 6, Itr 56000 / 60000: avg loss = 0.000002\n",
            "Epoch 6, Itr 57000 / 60000: avg loss = 0.000002\n",
            "Epoch 6, Itr 58000 / 60000: avg loss = 0.000002\n",
            "Epoch 6, Itr 59000 / 60000: avg loss = 0.000002\n",
            "Epoch 6 / Test accuracy: 96.29%\n",
            "Epoch 7, Itr 0 / 60000: avg loss = 0.000115\n",
            "Epoch 7, Itr 1000 / 60000: avg loss = 0.000002\n",
            "Epoch 7, Itr 2000 / 60000: avg loss = 0.000002\n",
            "Epoch 7, Itr 3000 / 60000: avg loss = 0.000002\n",
            "Epoch 7, Itr 4000 / 60000: avg loss = 0.000002\n",
            "Epoch 7, Itr 5000 / 60000: avg loss = 0.000002\n",
            "Epoch 7, Itr 6000 / 60000: avg loss = 0.000002\n",
            "Epoch 7, Itr 7000 / 60000: avg loss = 0.000002\n",
            "Epoch 7, Itr 8000 / 60000: avg loss = 0.000002\n",
            "Epoch 7, Itr 9000 / 60000: avg loss = 0.000002\n",
            "Epoch 7, Itr 10000 / 60000: avg loss = 0.000002\n",
            "Epoch 7, Itr 11000 / 60000: avg loss = 0.000002\n",
            "Epoch 7, Itr 12000 / 60000: avg loss = 0.000002\n",
            "Epoch 7, Itr 13000 / 60000: avg loss = 0.000002\n",
            "Epoch 7, Itr 14000 / 60000: avg loss = 0.000002\n",
            "Epoch 7, Itr 15000 / 60000: avg loss = 0.000002\n",
            "Epoch 7, Itr 16000 / 60000: avg loss = 0.000002\n",
            "Epoch 7, Itr 17000 / 60000: avg loss = 0.000002\n",
            "Epoch 7, Itr 18000 / 60000: avg loss = 0.000002\n",
            "Epoch 7, Itr 19000 / 60000: avg loss = 0.000002\n",
            "Epoch 7, Itr 20000 / 60000: avg loss = 0.000002\n",
            "Epoch 7, Itr 21000 / 60000: avg loss = 0.000002\n",
            "Epoch 7, Itr 22000 / 60000: avg loss = 0.000002\n",
            "Epoch 7, Itr 23000 / 60000: avg loss = 0.000002\n",
            "Epoch 7, Itr 24000 / 60000: avg loss = 0.000002\n",
            "Epoch 7, Itr 25000 / 60000: avg loss = 0.000002\n",
            "Epoch 7, Itr 26000 / 60000: avg loss = 0.000002\n",
            "Epoch 7, Itr 27000 / 60000: avg loss = 0.000002\n",
            "Epoch 7, Itr 28000 / 60000: avg loss = 0.000002\n",
            "Epoch 7, Itr 29000 / 60000: avg loss = 0.000002\n",
            "Epoch 7, Itr 30000 / 60000: avg loss = 0.000002\n",
            "Epoch 7, Itr 31000 / 60000: avg loss = 0.000002\n",
            "Epoch 7, Itr 32000 / 60000: avg loss = 0.000002\n",
            "Epoch 7, Itr 33000 / 60000: avg loss = 0.000002\n",
            "Epoch 7, Itr 34000 / 60000: avg loss = 0.000002\n",
            "Epoch 7, Itr 35000 / 60000: avg loss = 0.000002\n",
            "Epoch 7, Itr 36000 / 60000: avg loss = 0.000002\n",
            "Epoch 7, Itr 37000 / 60000: avg loss = 0.000002\n",
            "Epoch 7, Itr 38000 / 60000: avg loss = 0.000002\n",
            "Epoch 7, Itr 39000 / 60000: avg loss = 0.000002\n",
            "Epoch 7, Itr 40000 / 60000: avg loss = 0.000002\n",
            "Epoch 7, Itr 41000 / 60000: avg loss = 0.000002\n",
            "Epoch 7, Itr 42000 / 60000: avg loss = 0.000002\n",
            "Epoch 7, Itr 43000 / 60000: avg loss = 0.000002\n",
            "Epoch 7, Itr 44000 / 60000: avg loss = 0.000002\n",
            "Epoch 7, Itr 45000 / 60000: avg loss = 0.000002\n",
            "Epoch 7, Itr 46000 / 60000: avg loss = 0.000002\n",
            "Epoch 7, Itr 47000 / 60000: avg loss = 0.000002\n",
            "Epoch 7, Itr 48000 / 60000: avg loss = 0.000002\n",
            "Epoch 7, Itr 49000 / 60000: avg loss = 0.000002\n",
            "Epoch 7, Itr 50000 / 60000: avg loss = 0.000002\n",
            "Epoch 7, Itr 51000 / 60000: avg loss = 0.000002\n",
            "Epoch 7, Itr 52000 / 60000: avg loss = 0.000002\n",
            "Epoch 7, Itr 53000 / 60000: avg loss = 0.000002\n",
            "Epoch 7, Itr 54000 / 60000: avg loss = 0.000002\n",
            "Epoch 7, Itr 55000 / 60000: avg loss = 0.000002\n",
            "Epoch 7, Itr 56000 / 60000: avg loss = 0.000002\n",
            "Epoch 7, Itr 57000 / 60000: avg loss = 0.000002\n",
            "Epoch 7, Itr 58000 / 60000: avg loss = 0.000002\n",
            "Epoch 7, Itr 59000 / 60000: avg loss = 0.000002\n",
            "Epoch 7 / Test accuracy: 96.52%\n",
            "Epoch 8, Itr 0 / 60000: avg loss = 0.000196\n",
            "Epoch 8, Itr 1000 / 60000: avg loss = 0.000002\n",
            "Epoch 8, Itr 2000 / 60000: avg loss = 0.000002\n",
            "Epoch 8, Itr 3000 / 60000: avg loss = 0.000002\n",
            "Epoch 8, Itr 4000 / 60000: avg loss = 0.000002\n",
            "Epoch 8, Itr 5000 / 60000: avg loss = 0.000002\n",
            "Epoch 8, Itr 6000 / 60000: avg loss = 0.000002\n",
            "Epoch 8, Itr 7000 / 60000: avg loss = 0.000002\n",
            "Epoch 8, Itr 8000 / 60000: avg loss = 0.000002\n",
            "Epoch 8, Itr 9000 / 60000: avg loss = 0.000002\n",
            "Epoch 8, Itr 10000 / 60000: avg loss = 0.000002\n",
            "Epoch 8, Itr 11000 / 60000: avg loss = 0.000002\n",
            "Epoch 8, Itr 12000 / 60000: avg loss = 0.000002\n",
            "Epoch 8, Itr 13000 / 60000: avg loss = 0.000002\n",
            "Epoch 8, Itr 14000 / 60000: avg loss = 0.000002\n",
            "Epoch 8, Itr 15000 / 60000: avg loss = 0.000002\n",
            "Epoch 8, Itr 16000 / 60000: avg loss = 0.000002\n",
            "Epoch 8, Itr 17000 / 60000: avg loss = 0.000002\n",
            "Epoch 8, Itr 18000 / 60000: avg loss = 0.000002\n",
            "Epoch 8, Itr 19000 / 60000: avg loss = 0.000002\n",
            "Epoch 8, Itr 20000 / 60000: avg loss = 0.000002\n",
            "Epoch 8, Itr 21000 / 60000: avg loss = 0.000002\n",
            "Epoch 8, Itr 22000 / 60000: avg loss = 0.000002\n",
            "Epoch 8, Itr 23000 / 60000: avg loss = 0.000002\n",
            "Epoch 8, Itr 24000 / 60000: avg loss = 0.000002\n",
            "Epoch 8, Itr 25000 / 60000: avg loss = 0.000002\n",
            "Epoch 8, Itr 26000 / 60000: avg loss = 0.000002\n",
            "Epoch 8, Itr 27000 / 60000: avg loss = 0.000002\n",
            "Epoch 8, Itr 28000 / 60000: avg loss = 0.000002\n",
            "Epoch 8, Itr 29000 / 60000: avg loss = 0.000002\n",
            "Epoch 8, Itr 30000 / 60000: avg loss = 0.000002\n",
            "Epoch 8, Itr 31000 / 60000: avg loss = 0.000002\n",
            "Epoch 8, Itr 32000 / 60000: avg loss = 0.000002\n",
            "Epoch 8, Itr 33000 / 60000: avg loss = 0.000002\n",
            "Epoch 8, Itr 34000 / 60000: avg loss = 0.000002\n",
            "Epoch 8, Itr 35000 / 60000: avg loss = 0.000002\n",
            "Epoch 8, Itr 36000 / 60000: avg loss = 0.000002\n",
            "Epoch 8, Itr 37000 / 60000: avg loss = 0.000002\n",
            "Epoch 8, Itr 38000 / 60000: avg loss = 0.000002\n",
            "Epoch 8, Itr 39000 / 60000: avg loss = 0.000002\n",
            "Epoch 8, Itr 40000 / 60000: avg loss = 0.000002\n",
            "Epoch 8, Itr 41000 / 60000: avg loss = 0.000002\n",
            "Epoch 8, Itr 42000 / 60000: avg loss = 0.000002\n",
            "Epoch 8, Itr 43000 / 60000: avg loss = 0.000002\n",
            "Epoch 8, Itr 44000 / 60000: avg loss = 0.000002\n",
            "Epoch 8, Itr 45000 / 60000: avg loss = 0.000002\n",
            "Epoch 8, Itr 46000 / 60000: avg loss = 0.000002\n",
            "Epoch 8, Itr 47000 / 60000: avg loss = 0.000002\n",
            "Epoch 8, Itr 48000 / 60000: avg loss = 0.000001\n",
            "Epoch 8, Itr 49000 / 60000: avg loss = 0.000001\n",
            "Epoch 8, Itr 50000 / 60000: avg loss = 0.000001\n",
            "Epoch 8, Itr 51000 / 60000: avg loss = 0.000002\n",
            "Epoch 8, Itr 52000 / 60000: avg loss = 0.000001\n",
            "Epoch 8, Itr 53000 / 60000: avg loss = 0.000002\n",
            "Epoch 8, Itr 54000 / 60000: avg loss = 0.000001\n",
            "Epoch 8, Itr 55000 / 60000: avg loss = 0.000001\n",
            "Epoch 8, Itr 56000 / 60000: avg loss = 0.000002\n",
            "Epoch 8, Itr 57000 / 60000: avg loss = 0.000001\n",
            "Epoch 8, Itr 58000 / 60000: avg loss = 0.000001\n",
            "Epoch 8, Itr 59000 / 60000: avg loss = 0.000001\n",
            "Epoch 8 / Test accuracy: 96.75%\n",
            "Epoch 9, Itr 0 / 60000: avg loss = 0.000133\n",
            "Epoch 9, Itr 1000 / 60000: avg loss = 0.000001\n",
            "Epoch 9, Itr 2000 / 60000: avg loss = 0.000001\n",
            "Epoch 9, Itr 3000 / 60000: avg loss = 0.000001\n",
            "Epoch 9, Itr 4000 / 60000: avg loss = 0.000001\n",
            "Epoch 9, Itr 5000 / 60000: avg loss = 0.000001\n",
            "Epoch 9, Itr 6000 / 60000: avg loss = 0.000001\n",
            "Epoch 9, Itr 7000 / 60000: avg loss = 0.000001\n",
            "Epoch 9, Itr 8000 / 60000: avg loss = 0.000001\n",
            "Epoch 9, Itr 9000 / 60000: avg loss = 0.000001\n",
            "Epoch 9, Itr 10000 / 60000: avg loss = 0.000001\n",
            "Epoch 9, Itr 11000 / 60000: avg loss = 0.000001\n",
            "Epoch 9, Itr 12000 / 60000: avg loss = 0.000001\n",
            "Epoch 9, Itr 13000 / 60000: avg loss = 0.000001\n",
            "Epoch 9, Itr 14000 / 60000: avg loss = 0.000001\n",
            "Epoch 9, Itr 15000 / 60000: avg loss = 0.000001\n",
            "Epoch 9, Itr 16000 / 60000: avg loss = 0.000001\n",
            "Epoch 9, Itr 17000 / 60000: avg loss = 0.000001\n",
            "Epoch 9, Itr 18000 / 60000: avg loss = 0.000001\n",
            "Epoch 9, Itr 19000 / 60000: avg loss = 0.000001\n",
            "Epoch 9, Itr 20000 / 60000: avg loss = 0.000001\n",
            "Epoch 9, Itr 21000 / 60000: avg loss = 0.000001\n",
            "Epoch 9, Itr 22000 / 60000: avg loss = 0.000001\n",
            "Epoch 9, Itr 23000 / 60000: avg loss = 0.000001\n",
            "Epoch 9, Itr 24000 / 60000: avg loss = 0.000001\n",
            "Epoch 9, Itr 25000 / 60000: avg loss = 0.000001\n",
            "Epoch 9, Itr 26000 / 60000: avg loss = 0.000001\n",
            "Epoch 9, Itr 27000 / 60000: avg loss = 0.000001\n",
            "Epoch 9, Itr 28000 / 60000: avg loss = 0.000001\n",
            "Epoch 9, Itr 29000 / 60000: avg loss = 0.000001\n",
            "Epoch 9, Itr 30000 / 60000: avg loss = 0.000001\n",
            "Epoch 9, Itr 31000 / 60000: avg loss = 0.000001\n",
            "Epoch 9, Itr 32000 / 60000: avg loss = 0.000001\n",
            "Epoch 9, Itr 33000 / 60000: avg loss = 0.000001\n",
            "Epoch 9, Itr 34000 / 60000: avg loss = 0.000001\n",
            "Epoch 9, Itr 35000 / 60000: avg loss = 0.000001\n",
            "Epoch 9, Itr 36000 / 60000: avg loss = 0.000001\n",
            "Epoch 9, Itr 37000 / 60000: avg loss = 0.000001\n",
            "Epoch 9, Itr 38000 / 60000: avg loss = 0.000001\n",
            "Epoch 9, Itr 39000 / 60000: avg loss = 0.000001\n",
            "Epoch 9, Itr 40000 / 60000: avg loss = 0.000001\n",
            "Epoch 9, Itr 41000 / 60000: avg loss = 0.000001\n",
            "Epoch 9, Itr 42000 / 60000: avg loss = 0.000001\n",
            "Epoch 9, Itr 43000 / 60000: avg loss = 0.000001\n",
            "Epoch 9, Itr 44000 / 60000: avg loss = 0.000001\n",
            "Epoch 9, Itr 45000 / 60000: avg loss = 0.000001\n",
            "Epoch 9, Itr 46000 / 60000: avg loss = 0.000001\n",
            "Epoch 9, Itr 47000 / 60000: avg loss = 0.000001\n",
            "Epoch 9, Itr 48000 / 60000: avg loss = 0.000001\n",
            "Epoch 9, Itr 49000 / 60000: avg loss = 0.000001\n",
            "Epoch 9, Itr 50000 / 60000: avg loss = 0.000001\n",
            "Epoch 9, Itr 51000 / 60000: avg loss = 0.000001\n",
            "Epoch 9, Itr 52000 / 60000: avg loss = 0.000001\n",
            "Epoch 9, Itr 53000 / 60000: avg loss = 0.000001\n",
            "Epoch 9, Itr 54000 / 60000: avg loss = 0.000001\n",
            "Epoch 9, Itr 55000 / 60000: avg loss = 0.000001\n",
            "Epoch 9, Itr 56000 / 60000: avg loss = 0.000001\n",
            "Epoch 9, Itr 57000 / 60000: avg loss = 0.000001\n",
            "Epoch 9, Itr 58000 / 60000: avg loss = 0.000001\n",
            "Epoch 9, Itr 59000 / 60000: avg loss = 0.000001\n",
            "Epoch 9 / Test accuracy: 97.03%\n",
            "Epoch 10, Itr 0 / 60000: avg loss = 0.000105\n",
            "Epoch 10, Itr 1000 / 60000: avg loss = 0.000001\n",
            "Epoch 10, Itr 2000 / 60000: avg loss = 0.000001\n",
            "Epoch 10, Itr 3000 / 60000: avg loss = 0.000001\n",
            "Epoch 10, Itr 4000 / 60000: avg loss = 0.000001\n",
            "Epoch 10, Itr 5000 / 60000: avg loss = 0.000001\n",
            "Epoch 10, Itr 6000 / 60000: avg loss = 0.000001\n",
            "Epoch 10, Itr 7000 / 60000: avg loss = 0.000001\n",
            "Epoch 10, Itr 8000 / 60000: avg loss = 0.000001\n",
            "Epoch 10, Itr 9000 / 60000: avg loss = 0.000001\n",
            "Epoch 10, Itr 10000 / 60000: avg loss = 0.000001\n",
            "Epoch 10, Itr 11000 / 60000: avg loss = 0.000001\n",
            "Epoch 10, Itr 12000 / 60000: avg loss = 0.000001\n",
            "Epoch 10, Itr 13000 / 60000: avg loss = 0.000001\n",
            "Epoch 10, Itr 14000 / 60000: avg loss = 0.000001\n",
            "Epoch 10, Itr 15000 / 60000: avg loss = 0.000001\n",
            "Epoch 10, Itr 16000 / 60000: avg loss = 0.000001\n",
            "Epoch 10, Itr 17000 / 60000: avg loss = 0.000001\n",
            "Epoch 10, Itr 18000 / 60000: avg loss = 0.000001\n",
            "Epoch 10, Itr 19000 / 60000: avg loss = 0.000001\n",
            "Epoch 10, Itr 20000 / 60000: avg loss = 0.000001\n",
            "Epoch 10, Itr 21000 / 60000: avg loss = 0.000001\n",
            "Epoch 10, Itr 22000 / 60000: avg loss = 0.000001\n",
            "Epoch 10, Itr 23000 / 60000: avg loss = 0.000001\n",
            "Epoch 10, Itr 24000 / 60000: avg loss = 0.000001\n",
            "Epoch 10, Itr 25000 / 60000: avg loss = 0.000001\n",
            "Epoch 10, Itr 26000 / 60000: avg loss = 0.000001\n",
            "Epoch 10, Itr 27000 / 60000: avg loss = 0.000001\n",
            "Epoch 10, Itr 28000 / 60000: avg loss = 0.000001\n",
            "Epoch 10, Itr 29000 / 60000: avg loss = 0.000001\n",
            "Epoch 10, Itr 30000 / 60000: avg loss = 0.000001\n",
            "Epoch 10, Itr 31000 / 60000: avg loss = 0.000001\n",
            "Epoch 10, Itr 32000 / 60000: avg loss = 0.000001\n",
            "Epoch 10, Itr 33000 / 60000: avg loss = 0.000001\n",
            "Epoch 10, Itr 34000 / 60000: avg loss = 0.000001\n",
            "Epoch 10, Itr 35000 / 60000: avg loss = 0.000001\n",
            "Epoch 10, Itr 36000 / 60000: avg loss = 0.000001\n",
            "Epoch 10, Itr 37000 / 60000: avg loss = 0.000001\n",
            "Epoch 10, Itr 38000 / 60000: avg loss = 0.000001\n",
            "Epoch 10, Itr 39000 / 60000: avg loss = 0.000001\n",
            "Epoch 10, Itr 40000 / 60000: avg loss = 0.000001\n",
            "Epoch 10, Itr 41000 / 60000: avg loss = 0.000001\n",
            "Epoch 10, Itr 42000 / 60000: avg loss = 0.000001\n",
            "Epoch 10, Itr 43000 / 60000: avg loss = 0.000001\n",
            "Epoch 10, Itr 44000 / 60000: avg loss = 0.000001\n",
            "Epoch 10, Itr 45000 / 60000: avg loss = 0.000001\n",
            "Epoch 10, Itr 46000 / 60000: avg loss = 0.000001\n",
            "Epoch 10, Itr 47000 / 60000: avg loss = 0.000001\n",
            "Epoch 10, Itr 48000 / 60000: avg loss = 0.000001\n",
            "Epoch 10, Itr 49000 / 60000: avg loss = 0.000001\n",
            "Epoch 10, Itr 50000 / 60000: avg loss = 0.000001\n",
            "Epoch 10, Itr 51000 / 60000: avg loss = 0.000001\n",
            "Epoch 10, Itr 52000 / 60000: avg loss = 0.000001\n",
            "Epoch 10, Itr 53000 / 60000: avg loss = 0.000001\n",
            "Epoch 10, Itr 54000 / 60000: avg loss = 0.000001\n",
            "Epoch 10, Itr 55000 / 60000: avg loss = 0.000001\n",
            "Epoch 10, Itr 56000 / 60000: avg loss = 0.000001\n",
            "Epoch 10, Itr 57000 / 60000: avg loss = 0.000001\n",
            "Epoch 10, Itr 58000 / 60000: avg loss = 0.000001\n",
            "Epoch 10, Itr 59000 / 60000: avg loss = 0.000001\n",
            "Epoch 10 / Test accuracy: 97.15%\n"
          ]
        }
      ],
      "source": [
        "# Initialize network parameters\n",
        "class MLP3:\n",
        "    def __init__(self, in_dim=784, h1=128, h2=64, out_dim=10, seed=42):\n",
        "        rng = np.random.default_rng(seed)\n",
        "        self.W1 = Tensor(rng.normal(0, 0.01, (in_dim, h1)), requires_grad=True)\n",
        "        self.b1 = Tensor(np.zeros(h1, dtype=np.float32), requires_grad=True)\n",
        "        self.W2 = Tensor(rng.normal(0, 0.01, (h1, h2)), requires_grad=True)\n",
        "        self.b2 = Tensor(np.zeros(h2, dtype=np.float32), requires_grad=True)\n",
        "        self.W3 = Tensor(rng.normal(0, 0.01, (h2, out_dim)), requires_grad=True)\n",
        "        self.b3 = Tensor(np.zeros(out_dim, dtype=np.float32), requires_grad=True)\n",
        "\n",
        "    @property\n",
        "    def params(self):\n",
        "        return [self.W1, self.b1, self.W2, self.b2, self.W3, self.b3]\n",
        "\n",
        "    def __call__(self, x: Tensor) -> Tensor:\n",
        "        h1 = ReLU.apply(x @ self.W1 + self.b1)\n",
        "        h2 = ReLU.apply(h1 @ self.W2 + self.b2)\n",
        "        logits = h2 @ self.W3 + self.b3\n",
        "        return logits\n",
        "\n",
        "model = MLP3()\n",
        "learning_rate = 0.1\n",
        "batch_size = 100\n",
        "num_epochs = 10\n",
        "weight_decay = 1e-4\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Shuffle training data\n",
        "    perm = np.random.permutation(X_train.shape[0])\n",
        "    X_train, y_train = X_train[perm], y_train[perm]\n",
        "\n",
        "    total_loss = 0.0\n",
        "    num_updates = 0\n",
        "\n",
        "    for i in range(0, X_train.shape[0], batch_size):\n",
        "        # Mini-batch slice\n",
        "        X_batch = Tensor(X_train[i:i+batch_size])       # input batch\n",
        "        y_batch = Tensor(y_train[i:i+batch_size])       # target batch\n",
        "\n",
        "        # Forward pass:\n",
        "        logits = model(X_batch)\n",
        "        probs  = Softmax.apply(logits)\n",
        "        logp   = Log.apply(probs)\n",
        "\n",
        "        data_loss   = NLLLoss.apply(logp, y_batch)\n",
        "        #loss = CrossEntropyLoss.apply(logits, y_batch)  # Compute cross-entropy loss over the batch\n",
        "        l2_W1 = Sum.apply(Mul.apply(model.W1, model.W1))\n",
        "        l2_W2 = Sum.apply(Mul.apply(model.W2, model.W2))\n",
        "        l2_W3 = Sum.apply(Mul.apply(model.W3, model.W3))\n",
        "        l2_sum = Add.apply(Add.apply(l2_W1, l2_W2), l2_W3)\n",
        "\n",
        "        reg_coef = Tensor(weight_decay / (2.0 * batch_size))\n",
        "        reg = Mul.apply(l2_sum, reg_coef)\n",
        "\n",
        "        loss = Add.apply(data_loss, reg)\n",
        "\n",
        "        total_loss += float(loss.data)\n",
        "        num_updates += 1\n",
        "\n",
        "        # Backward pass:\n",
        "        loss.backward()    # compute gradients for all weights/biases\n",
        "\n",
        "        # Update parameters with SGD:\n",
        "        for param in model.params:\n",
        "            # simple gradient descent step\n",
        "            param.data -= learning_rate * param.grad\n",
        "            # reset gradient to zero for next batch\n",
        "            param.grad = None\n",
        "\n",
        "        # Evaluate accuracy on the training set (optional) or just print loss\n",
        "        if ((i) % 1000) == 0:\n",
        "            print(f\"Epoch {epoch+1}, Itr {i} / {X_train.shape[0]}: avg loss = {total_loss / (X_train.shape[0] / batch_size * (i+1)):.6f}\")\n",
        "\n",
        "    # quick test accuracy\n",
        "    Xt = Tensor(X_test, requires_grad=False)\n",
        "    logits = model(Xt)\n",
        "    preds = np.argmax(logits.data, axis=1)\n",
        "    true  = np.argmax(y_test, axis=1)\n",
        "    acc = (preds == true).mean()\n",
        "    print(f\"Epoch {epoch+1} / Test accuracy: {acc*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c6ba184-b81e-46b6-a59b-7602840f025b",
      "metadata": {
        "id": "6c6ba184-b81e-46b6-a59b-7602840f025b"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "257af673-1864-47c0-a3ac-ae9b29be8ecc",
      "metadata": {
        "id": "257af673-1864-47c0-a3ac-ae9b29be8ecc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}